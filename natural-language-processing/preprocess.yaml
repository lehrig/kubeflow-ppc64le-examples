name: Preprocess
inputs:
- {name: dataset_dir, type: String}
outputs:
- {name: preprocess_dir, type: String}
implementation:
  container:
    image: quay.io/jeremie_ch/transformers-component:gpu
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def _make_parent_dirs_and_return_path(file_path: str):
          import os
          os.makedirs(os.path.dirname(file_path), exist_ok=True)
          return file_path

      def preprocess(dataset_dir,
                     preprocess_dir):

          from transformers import AutoTokenizer
          from datasets.load import load_from_disk
          from datasets import load_dataset
          import pickle
          import os

          tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

          # with open(dataset_dir + "/dataset.pkl", "rb") as f:
          #     squad = pickle.load(f)
          squad = load_from_disk(dataset_dir)

          def preprocess_function(examples):
              questions = [q.strip() for q in examples["question"]]
              inputs = tokenizer(
                  questions,
                  examples["context"],
                  max_length=384,
                  truncation="only_second",
                  return_offsets_mapping=True,
                  padding="max_length",
              )

              offset_mapping = inputs.pop("offset_mapping")
              answers = examples["answers"]
              start_positions = []
              end_positions = []

              for i, offset in enumerate(offset_mapping):
                  answer = answers[i]
                  start_char = answer["answer_start"][0]
                  end_char = answer["answer_start"][0] + len(answer["text"][0])
                  sequence_ids = inputs.sequence_ids(i)

                  # Find the start and end of the context
                  idx = 0
                  while sequence_ids[idx] != 1:
                      idx += 1
                  context_start = idx
                  while sequence_ids[idx] == 1:
                      idx += 1
                  context_end = idx - 1

                  # If the answer is not fully inside the context, label it (0, 0)
                  if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
                      start_positions.append(0)
                      end_positions.append(0)
                  else:
                      # Otherwise it's the start and end token positions
                      idx = context_start
                      while idx <= context_end and offset[idx][0] <= start_char:
                          idx += 1
                      start_positions.append(idx - 1)

                      idx = context_end
                      while idx >= context_start and offset[idx][1] >= end_char:
                          idx -= 1
                      end_positions.append(idx + 1)

              inputs["start_positions"] = start_positions
              inputs["end_positions"] = end_positions
              return inputs

          tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)

          if not os.path.exists(preprocess_dir):
              os.makedirs(preprocess_dir)

          tokenized_squad.save_to_disk(preprocess_dir)

      import argparse
      _parser = argparse.ArgumentParser(prog='Preprocess', description='')
      _parser.add_argument("--dataset-dir", dest="dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--preprocess-dir", dest="preprocess_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parsed_args = vars(_parser.parse_args())

      _outputs = preprocess(**_parsed_args)
    args:
    - --dataset-dir
    - {inputPath: dataset_dir}
    - --preprocess-dir
    - {outputPath: preprocess_dir}
